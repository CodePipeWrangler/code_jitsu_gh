#!/usr/bin/env python3

# Tandem Repeat Data Statistics
# Calculate and visualize GC content

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import argparse
from sklearn.preprocessing import StandardScaler

# Sequence-based features: GC content
def gc_content(seq):
    seq = seq.upper()
    return (seq.count("G") + seq.count("C")) / len(seq)

# Read the file and return the dataframe
def read_file(filename):
    return pd.read_csv(filename, sep="\t", names=['chr', 'pos', 'len', 'period', 'score', 'sub', 'ins', 'del', 'cons', 'seq'], usecols=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

# Normalize or scale the feature matrix
def normalize_features(df):
    X_numeric = df[['pos', 'len', 'period', 'score', 'sub', 'ins', 'del']].values
    scaler = StandardScaler()
    return scaler.fit_transform(X_numeric)

# Add GC content to the dataframe
def add_gc_content(df):
    df["gc_content"] = df["seq"].apply(gc_content)

# Create bins from positions
def create_bins(df, bin_size=1_000_000):
    df["bin"] = (df["pos"] // bin_size) * bin_size  # start of bin

# Group by bin and compute mean GC content
def group_by_bin(df):
    return df.groupby("bin")["gc_content"].mean().reset_index()

# Optional: Count number of repeats per bin
def count_repeats_per_bin(df, gc_by_bin):
    gc_by_bin["repeat_count"] = df.groupby("bin")["gc_content"].count().values
    return gc_by_bin

# Plot GC content by bin and save the figure
def plot_gc_content_by_bin(gc_by_bin, bin_size, i):
    plt.figure(figsize=(12, 5))
    plt.plot(gc_by_bin["bin"], gc_by_bin["gc_content"], marker="o")
    plt.title(f"Mean GC Content per {bin_size:,} bp Bin")
    # scale X-axis manually
    plt.xticks(ticks=gc_by_bin["bin"][::10], labels=(gc_by_bin["bin"][::10] // 1_000_000))
    plt.xlabel("Genomic Position (Mb)")
    plt.ylabel("Mean GC Content")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f"Chr.{i}_gc_content_by_bin.png", dpi=300)
    plt.cla()

# Main processing loop
def process_files(num_files, filename_pattern):
    for i in range(1, num_files + 1):
        filename = filename_pattern.replace("#", str(i))
        print(f"Processing file: {filename}")

        # Step 1: Read the file
        df = read_file(filename)

        # Step 2: Normalize features
        X_scaled = normalize_features(df)

        # Step 3: Add GC content
        add_gc_content(df)

        # Step 4: Add GC content to the feature matrix
        X_full = np.hstack([X_scaled, df[["gc_content"]].values])

        # Step 5: Create bins
        bin_size = 1_000_000
        create_bins(df, bin_size)

        # Step 6: Group by bin and calculate mean GC content
        gc_by_bin = group_by_bin(df)

        # Step 7: Optional: Count repeats per bin
        gc_by_bin = count_repeats_per_bin(df, gc_by_bin)

        # Step 8: Plot and save
        plot_gc_content_by_bin(gc_by_bin, bin_size, i)

# Command-line argument parsing
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process genomic data files.")
    parser.add_argument("num_files", type=int, help="The number of files to process.")
    parser.add_argument("filename_pattern", type=str, help="The filename pattern with '#' as a placeholder for file number.")
    
    args = parser.parse_args()

    # Call the main processing function
    process_files(args.num_files, args.filename_pattern)


